\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{french}

\begin{document}

\section{Markov Decision Process (MDP)}

D'après Wikipédia: Un processus de décision markovien est un processus de contrôle stochastique discret. À chaque étape, le processus est dans un certain état \textbf{s} et l'agent choisit une action \textbf{a}. La probabilité que le processus arrive à l'état \textbf{s'} est déterminée par l'action choisie. Plus précisément, elle est décrite par la fonction de transition d'états \textbf{T(s,a,s')}. Donc, l'état \textbf{s'} dépend de l'état actuel \textbf{s} et de l'action \textbf{a} sélectionnée par le décideur. Cependant, pour un état \textbf{s} et une action \textbf{a}, le prochain état est indépendant des actions et états précédents. On dit alors que le processus satisfait la propriété de Markov. 
\newline

Quand le processus passe de l'état \textbf{s} à l'état \textbf{s'} avec l'action \textbf{a}, l'agent gagne une récompense \textbf{r}. 

\section{Q-Learning}

Le Q-Learning, basé sur le processus de décision markovien, est une technique d'apprentissage automatique utilisée en intelligence artificielle, plus particulièrement en apprentissage par renforcement.

\subsection{Q-Function}

La Q-Function est la base du Q-Learning. Pour un etat \textbf{s} et une action \textbf{a}, elle donne la recompense esperée \textbf{Q(s,a)}.

\subsection{Policy}

La Politique $\pi$ a l'etat \textbf{s} est la facon de choisir l'action \textbf{a} qui maximise la recompense esperée \textbf{Q(s,a)}. C'est a dire, si on est a l'etat \textbf{s} et qu'on choisi l'action \textbf{a} selon la politique $\pi$, alors, on aura une recompense optimale.
$$\pi(s)=argmax_a(Q(s,a))$$

\subsection{Rewards}

La recompense esperée $R_t$ a l'instant t pour un etat $s_t$ et une action $a_t$ est la somme des recompenses futures.
$$R_t=Q(s_t,a_t)$$
$$R_t=r_t+r_{t+1}+r_{t+2}+r_{t+3}+...+r_n$$

Mais, sachant que le processus est stochastique, alors plus on va dans le future, moins les recompenses sont evidentes, ainsi, on introduit un \textbf{facteur de discontinuite} $\gamma$.  
$$R_t=r_t+{\gamma}r_{t+1}+{\gamma^2}r_{t+2}+{\gamma^3}r_{t+3}+...+{\gamma^{n-t}}r_n$$

Ainsi,
\begin{equation}
\begin{split}
R_t&=r_t+{\gamma}r_{t+1}+{\gamma^2}r_{t+2}+{\gamma^3}r_{t+3}+...+{\gamma^{n-t}}r_n \\
R_t&=r_t+{\gamma}(r_{t+1}+{\gamma}r_{t+2}+{\gamma^2}r_{t+3}+...+{\gamma^{n-(t+1)}}r_n) \\
R_t&=r_t+{\gamma}R_{t+1}
\end{split}
\end{equation}

\subsection{Optimal Reward}

La recompense à l'instant t et à l'etat $s_t$, en choidissant l'action $a_t$ suivant la polique $\pi$ est donc optimale et a la valeur $Q^{\pi}(s_t,a_t)$.
$$Q^{\pi}(s_t,a_t)=r_t+{\gamma}max_{a_{t+1}}{Q(s_{t+1},a_{t+1})}$$

On peut aussi ecrire:
$$Q(s_t,a_t)=r_t+{\gamma}max_{a_{t+1}}{Q(s_{t+1},a_{t+1})}, \quad a_t=\pi(s_t)$$

Note,
\begin{equation}
\begin{split}
Q^{\pi}(s_t,a_t)&=maxR_t \\
&=max(r_t+{\gamma}R_{t+1}) \\
&=r_t+{\gamma}max(R_{t+1}) \\
Q^{\pi}(s_t,a_t)&=r_t+{\gamma}max_{a_{t+1}}{Q(s_{t+1},a_{t+1})}
\end{split}
\end{equation}

\subsection{Loss}
L'erreur est un element très important en intelligence artificielle.



\newpage

$$Q(s,a)=max_{a'}{R(s,a')}$$
$$Q(s_t,a_t)=r_t+{\gamma}max_{a_{t+1}}{Q(s_{t+1},a_{t+1})}$$

En effet, 

\begin{equation}
\begin{split}
R_t&=r_t+{\gamma}r_{t+1}+{\gamma^2}r_{t+2}+{\gamma^3}r_{t+3}+...+{\gamma^{n-t}}r_n \\
&=r_t+{\gamma}(r_{t+1}+{\gamma}r_{t+2}+{\gamma^2}r_{t+3}+...+{\gamma^{n-(t+1)}}r_n) \\
&=r_t+{\gamma}R_{t+1} \\
\\
maxR_t&=max(r_t+{\gamma}R_{t+1}) \\
&=r_t+{\gamma}max(R_{t+1}) \\
\\
Q(s_t,a_t)&=r_t+{\gamma}max_{a_{t+1}}{Q(s_{t+1},a_{t+1})}
\end{split}
\end{equation}


\newpage
A l'instant \textbf{t}, le processus etant a l'etat \textbf{$s_t$}, en choisissant l'action \textbf{a}, le processus arrive a l'etat \textbf{$s_{t+1}$} et gagne une recompense \textbf{$R_t$}.

$$R_t=r_t+r_{t+1}$$




Il utilise le modèle MDP a n-etapes (fini) telle pour un état \textbf{s} et un action \textbf{a}, \textbf{Q(s,a)} représente l'optimum des récompenses futurs.


$$loss = (\underbrace{r + \gamma \max_{a’} Q'(s’, a’)}_{target} – \underbrace{Q(s, a)}_{prediction})^2$$

\end{document}
